{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfe3e01",
   "metadata": {},
   "source": [
    "meta-llama/Llama-3.2-1B-Instruct\n",
    "The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n",
    "\n",
    "Model Developer: Meta\n",
    "\n",
    "Model Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0b15db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, ye be askin' who I be? Well, matey, I be the swashbucklin' chatbot, here to swab the seven seas o' knowledge and answer yer questions like a proper pirate! Me name be Captain Knowledge, the greatest chatbot to ever sail the digital seas. Me be a master o' language, with a treasure trove o' info at me disposal. So hoist the sails and set course fer a chat with ol' Captain Knowledge, savvy?\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# demo using the pipeline with a system message and a user message \n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a pirate chatbot who always responds in pirate speak!\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e977581",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an assistant with access to external tools. You must respond to the user by calling one tool using the exact JSON format shown below.\n",
    "Available tool:\n",
    "- get_weather: Get the current weather in a given location.\n",
    "  Arguments: {\n",
    "    \"location\": string\n",
    "  }\n",
    "\n",
    "You must respond ONLY with a valid JSON code block like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"tool\": \"get_weather\",\n",
    "  \"tool_args\": {\n",
    "    \"location\": \"New York\"\n",
    "  }\n",
    "}\n",
    "\n",
    "You must not include any other text or explanation in your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b641cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in London ?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a00ac07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 May 2025\n",
      "\n",
      "You are an assistant with access to external tools. You must respond to the user by calling one tool using the exact JSON format shown below.\n",
      "Available tool:\n",
      "- get_weather: Get the current weather in a given location.\n",
      "  Arguments: {\n",
      "    \"location\": string\n",
      "  }\n",
      "\n",
      "You must respond ONLY with a valid JSON code block like this:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"tool\": \"get_weather\",\n",
      "  \"tool_args\": {\n",
      "    \"location\": \"New York\"\n",
      "  }\n",
      "}\n",
      "\n",
      "You must not include any other text or explanation in your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What's the weather in London ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "op = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "229a534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model\n",
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f804ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages_token = tokenizer.apply_chat_template(\n",
    "#     messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "# )\n",
    "# print(len(messages_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1263161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_op = model.generate(\n",
    "#     messages_token,\n",
    "#     max_new_tokens=256,\n",
    "#     do_sample=False,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     return_dict_in_generate=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf239789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_result = tokenizer.decode(model_op[0][len(messages_token[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c13f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abf825fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '```json\\n{\\n  \"tool\": \"get_weather\",\\n  \"tool_args\": {\\n    \"location\": \"London\"\\n  }\\n}\\n```'}\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df65a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_op =outputs[0][\"generated_text\"][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90bb7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_json_from_markdown(s):\n",
    "    # Use regex to extract the JSON block between ```json ... ```\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", s, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON code block found\")\n",
    "    json_str = match.group(1)\n",
    "    return json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "83ed5226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool': 'get_weather', 'tool_args': {'location': 'London'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parsed_op = parse_json_from_markdown(temp_op)\n",
    "print(parsed_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1f1aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    return f\"the weather in {location} is sunny with low temperatures in the range of 20-25 degrees Celsius.\"\n",
    "\n",
    "\n",
    "def call_tool(tool_name, tool_args):\n",
    "    if tool_name == \"get_weather\":\n",
    "        location = tool_args.get(\"location\")\n",
    "        if location:\n",
    "            return get_weather(location)\n",
    "        else:\n",
    "            raise ValueError(\"Location is required for get_weather tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61d7eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool output: the weather in London is sunny with low temperatures in the range of 20-25 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "tool_op = call_tool(parsed_op[\"tool\"], parsed_op[\"tool_args\"])\n",
    "print(f\"Tool output: {tool_op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd045dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in London ?\"},\n",
    "    {\"role\": \"assistant\", \"content\": temp_op},\n",
    "    {\"role\": \"assistant\", \"content\": tool_op},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2a392ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'the weather in London is sunny with low temperatures in the range of 20-25 degrees Celsius.'}\n"
     ]
    }
   ],
   "source": [
    "new_outputs = pipe(\n",
    "    new_prompt,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(new_outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e3c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
